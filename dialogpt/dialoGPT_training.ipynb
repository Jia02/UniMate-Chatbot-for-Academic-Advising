{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "V1VlATWdBG9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch]\n",
        "!pip install accelerate -U"
      ],
      "metadata": {
        "id": "qmD3nZrhWFrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import List\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import PreTrainedTokenizer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ],
      "metadata": {
        "id": "W2CtT2aYWJ1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DatasetFromJSON(Dataset):\n",
        "    def __init__(self, data: List[List[str]], tokenizer: PreTrainedTokenizer, max_length=1024):\n",
        "      \"\"\"\n",
        "        Initializes a dataset from JSON data for training a conversational model.\n",
        "\n",
        "        Args:\n",
        "            data (List[List[str]]): A list of conversation sequences.\n",
        "            tokenizer (PreTrainedTokenizer): Tokenizer for encoding conversation pairs.\n",
        "            max_length (int): Maximum sequence length after tokenization.\n",
        "      \"\"\"\n",
        "      self.tokenizer = tokenizer\n",
        "      self.input_data = []\n",
        "      self.max_length = max_length\n",
        "      for conversation in data:\n",
        "          for i in range(len(conversation) - 1):\n",
        "              input_pair = (conversation[i], conversation[i + 1])\n",
        "              encoded_pair = tokenizer.encode(input_pair[0], input_pair[1], add_special_tokens=True, truncation=True, max_length=self.max_length, padding=\"max_length\")\n",
        "              self.input_data.append(encoded_pair)\n",
        "\n",
        "    def __len__(self):\n",
        "      # Returns the total number of encoded conversation pairs in the dataset.\n",
        "      return len(self.input_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      # Returns the encoded conversation pair at the specified index.\n",
        "      example = self.input_data[idx]\n",
        "      return example"
      ],
      "metadata": {
        "id": "A4EQv2y6WMIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_process_json(file_path: str) -> List[List[str]]:\n",
        "  \"\"\"\n",
        "    Reads a JSON file and processes its content into a list of conversation sequences.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        List[List[str]]: A list of conversation sequences.\n",
        "  \"\"\"\n",
        "  print(\"Reading...\")\n",
        "  with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "      data = json.load(file)\n",
        "  return data\n",
        "\n",
        "# Configure the training hyperparameters\n",
        "def train_dialogpt(model_name, train_data, output_dir, epochs):\n",
        "  \"\"\"\n",
        "    Trains a DialoGPT-small model for dialog generation using specified hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        model_name (str): Name of the pre-trained DialoGPT-small model.\n",
        "        train_data (List[List[str]]): List of conversation sequences.\n",
        "        output_dir (str): Directory to save the trained model and logs.\n",
        "        epochs (int): Number of training epochs.\n",
        "  \"\"\"\n",
        "  print(\"Training...\")\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "  model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "  model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "  train_dataset = DatasetFromJSON(train_data, tokenizer)\n",
        "\n",
        "  data_collator = DataCollatorForLanguageModeling(\n",
        "      tokenizer=tokenizer,\n",
        "      mlm=False,\n",
        "      pad_to_multiple_of=8\n",
        "  )\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "      output_dir=output_dir,\n",
        "      overwrite_output_dir=True,\n",
        "      num_train_epochs=epochs,\n",
        "      per_device_train_batch_size=1,\n",
        "      save_steps=100,\n",
        "      save_total_limit=3,\n",
        "      logging_steps=100,\n",
        "  )\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      data_collator=data_collator,\n",
        "      train_dataset=train_dataset,\n",
        "  )\n",
        "  trainer.train()\n",
        "\n",
        "# Define a function to train the model on training dataset\n",
        "def train_model(training_file, model_output):\n",
        "    train_data = read_and_process_json(training_file)\n",
        "    train_dialo_gpt(\"microsoft/DialoGPT-small\", train_data, model_output, 100)"
      ],
      "metadata": {
        "id": "qGYcGI68WO1x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remarks before training** : manually create two folders:\n",
        "- one folder named 'training' and ensure the training dataset (json formart) is loaded into the folder.\n",
        "- one folder named 'models' to save the depedencies of the fine-tuned model and tokenizer.\n",
        "\n",
        "**Remarks after training**:\n",
        "- save the checkpoint folder after training."
      ],
      "metadata": {
        "id": "xBPkDsouEMMa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_file = \"/content/training /training_data.json\" # directory that conatins training dataset\n",
        "  model_output = \"/content/models\" # directory that save the trained model and tokenixer\n",
        "\n",
        "  train_model(training_file, model_output) # train the model\n"
      ],
      "metadata": {
        "id": "1GZQ4VCCWWQy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}