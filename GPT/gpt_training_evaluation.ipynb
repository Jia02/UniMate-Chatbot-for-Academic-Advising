{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNz1gzjorQ8L"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1zhCOTVU0ss",
        "outputId": "0b434d75-dc40-4497-da7f-d3f3fed07e43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Optional\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08z3xfDXbi5K"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HysLPjcb8XL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DSFAmEhDcBO7"
      },
      "outputs": [],
      "source": [
        "def read_txt(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def read_documents_from_directory(directory):\n",
        "    combined_text = \"\"\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        if filename.endswith(\".txt\"):\n",
        "            combined_text += read_txt(file_path)\n",
        "    return combined_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtYCM8PGum5u"
      },
      "outputs": [],
      "source": [
        "pip install transformers[torch]\n",
        "pip install accelerate -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Na7NCjuEOTY"
      },
      "outputs": [],
      "source": [
        "#Configure the training hyperparameters\n",
        "\n",
        "import logging\n",
        "import os\n",
        "\n",
        "def train_chatbot(directory, model_output_path):\n",
        "    # Read documents from the directory\n",
        "    combined_text = read_documents_from_directory(directory)\n",
        "    combined_text = re.sub(r'\\n+', '\\n', combined_text).strip()  # Remove excess newline characters\n",
        "    train_text = combined_text\n",
        "\n",
        "    # Save the training data as text files with utf-8 encoding\n",
        "    with open(\"train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(train_text)\n",
        "\n",
        "    # Set up the tokenizer and model\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "    model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "    # Prepare the dataset\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=128)\n",
        "\n",
        "    # helper object used to be initialized with a tokenizer and set to handle masked language modeling (MLM) tasks.\n",
        "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "    # Set up the training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=model_output_path,\n",
        "        overwrite_output_dir=True,\n",
        "        per_device_train_batch_size=4,\n",
        "\n",
        "        num_train_epochs=100,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        logging_dir='/content/logs',\n",
        "        logging_steps=200,\n",
        "        logging_first_step=True,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_dataset,\n",
        "        # eval_dataset=val_dataset,\n",
        "\n",
        "    )\n",
        "\n",
        "    # Extract the log file name from the given path\n",
        "    log_file_name = os.path.basename(training_args.logging_dir)\n",
        "\n",
        "    # Configure the logging module to save logs in UTF-8 format\n",
        "    logging.basicConfig(\n",
        "        filename=os.path.join(training_args.logging_dir, log_file_name),\n",
        "        filemode='w',\n",
        "        level=logging.DEBUG,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        encoding='utf-8'\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(model_output_path)\n",
        "\n",
        "    # Save the tokenizer\n",
        "    tokenizer.save_pretrained(model_output_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhrkcXR_cWFK"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    directory = \"/content/drive/MyDrive/0Colab_Notebooks/0jia_gpt_test6/training\" # directory that contains the training dataset file\n",
        "    model_output_path = \"/content/gpt2model\" # directory to store the fine-tuned model and tokenizer\n",
        "\n",
        "    # Train the chatbot\n",
        "    train_chatbot(directory, model_output_path)\n",
        "\n",
        "    # Load the fine-tuned model and tokenizer\n",
        "    model = GPT2LMHeadModel.from_pretrained(model_output_path)\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(model_output_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8yNfxIFiB6V"
      },
      "source": [
        "# Generate response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEb6sMhTSSu6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f76hbfurjntH"
      },
      "outputs": [],
      "source": [
        "def generate_response(model, tokenizer, prompt, max_length):\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "    # Create the attention mask and pad token id\n",
        "    attention_mask = torch.ones_like(input_ids)\n",
        "    pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    # Configure the hyperparameters when generating response\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        num_return_sequences=1,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=pad_token_id,\n",
        "        do_sample=True,\n",
        "        temperature=0.3,\n",
        "        top_k=20,\n",
        "        top_p=0.2,\n",
        "        repetition_penalty=1.0\n",
        "    )\n",
        "\n",
        "    # Remove the prompt from the generated output\n",
        "    generated_text = tokenizer.decode(output[0])[len(prompt):].strip().strip('\"').strip(',')\n",
        "\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Remark:** Fine-tuned model and tokenizer's directory: https://drive.google.com/drive/folders/1myVJhNC6R7wIznBGxIn-jJdlDf3C4rQn?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-6RE2QUtoGs",
        "outputId": "7aa8c3e2-db44-44a0-ebb8-9a47a90bf87b"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/0Colab_Notebooks/0jia_gpt_test6/gpt2model\" # directory that contains the fine-tuned model\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Enter the prompt that wanted to ask the fine-tuned model\n",
        "prompt = \"What are the educational goals of Sunway University, and how are they elaborated upon?\"\n",
        "\n",
        "# Load the response generated by the fine-tuned model\n",
        "response = generate_response(my_chat_model, my_chat_tokenizer, prompt, 150)\n",
        "print(\"Generated response:\", response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKU64WR7qMNt"
      },
      "source": [
        "# Evaluate the performance of the fine-tuned model based on several performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtzqJnU5ggb-"
      },
      "outputs": [],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqLROR6TeGK_"
      },
      "outputs": [],
      "source": [
        "# Define a function to calculate similarity scores (Jaccard Similarity score, BLEU score and ROUGE score)\n",
        "def calculate_similarity_score(response, correct_answer):\n",
        "    jaccard_similarity = len(set(response).intersection(correct_answer)) / len(set(response).union(correct_answer))\n",
        "    bleu_score = sentence_bleu([correct_answer.split()], response.split())\n",
        "    rouge_scores = rouge.get_scores(response, correct_answer)\n",
        "    return jaccard_similarity, bleu_score, rouge_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewjZ0OEVTZvo",
        "outputId": "4b4d6f13-00ba-46e1-c3d1-09f49d8d8803"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from rouge import Rouge\n",
        "from collections import Counter\n",
        "\n",
        "# Run the main function to calculate cimilarity scores\n",
        "if __name__ == \"__main__\":\n",
        "  rouge = Rouge()\n",
        "\n",
        "  model_path = \"/content/drive/MyDrive/0Colab_Notebooks/0jia_gpt_test6/gpt2model\" # path that contains the fine-tuned model\n",
        "  test_path = \"/content/drive/MyDrive/0Colab_Notebooks/0jia_gpt_test6/testing/rephrasedQ&A.txt\" # path that contains the testing dataset file\n",
        "\n",
        "  # Load the fine-tuned model and tokenizer\n",
        "  my_chat_model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "  my_chat_tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n",
        "  test_questions = []\n",
        "  groundtruth = []\n",
        "  bot_responses = []\n",
        "  response_times = []\n",
        "  jaccard_similarities = []\n",
        "  bleu_scores = []\n",
        "  rouge_1_scores = []\n",
        "  rouge_2_scores = []\n",
        "  rouge_l_scores = []\n",
        "\n",
        "  # Read the testing dataset\n",
        "  with open(test_path, 'r') as file:\n",
        "    test_data = eval(file.read())\n",
        "\n",
        "  # Determine a list to store the question/prompt and validate answer/response extracted from the testing dataset\n",
        "  for item in test_data:\n",
        "      question = item[0]\n",
        "      answer = item[1]\n",
        "      test_questions.append(question)\n",
        "      groundtruth.append(answer)\n",
        "\n",
        "  # Evaluate the similarities between the generated response from the chatbot and validate response\n",
        "  for question, correct_answer in zip(test_questions, groundtruth):\n",
        "    start_time = timeit.default_timer() # start time\n",
        "    response = generate_response(my_chat_model, my_chat_tokenizer, question, 150) # generate a response based on the prompt ask\n",
        "    end_time = timeit.default_timer() # stop time\n",
        "\n",
        "    # Append the response to a list\n",
        "    bot_responses.append(response)\n",
        "\n",
        "    # Measure the duration of generating a response\n",
        "    response_times.append(end_time - start_time)\n",
        "\n",
        "    # Call the function to calculate respective similarity scores\n",
        "    jaccard_similarity, bleu_score, rouge_scores = calculate_similarity_score(response, correct_answer)\n",
        "\n",
        "    # Append the similarity scores to respective lists\n",
        "    jaccard_similarities.append(jaccard_similarity)\n",
        "    bleu_scores.append(bleu_score)\n",
        "    rouge_1_scores.append(rouge_scores[0]['rouge-1']['f'])\n",
        "    rouge_2_scores.append(rouge_scores[0]['rouge-2']['f'])\n",
        "    rouge_l_scores.append(rouge_scores[0]['rouge-l']['f'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gkm-wqmdzl3",
        "outputId": "a028b165-7b38-4270-9783-2c2c5d4cff34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.42658009173765415\n",
            "Recall: 0.48923959827833574\n",
            "F1 Score: 0.45576629327902246\n"
          ]
        }
      ],
      "source": [
        "# Define a list of:\n",
        "# num_c : to store the number of tokens that shared between gold and predicted answers,\n",
        "# num_p : to store the number of predicted tokens,\n",
        "# num_p : to store the number of ground truth tokens\n",
        "num_c = []\n",
        "num_p = []\n",
        "num_g = []\n",
        "num_total = len(bot_responses)\n",
        "\n",
        "# Calculate the number of common tokens that shared between ground truth and predicted answers,\n",
        "# the number of predicted tokens, and the number of ground truth tokens.\n",
        "for a in range(num_total):\n",
        "    common = Counter(groundtruth[a].split()) & Counter(bot_responses[a].split())  # tokens shared between ground truth and predicted tokens\n",
        "    num_common = sum(common.values())\n",
        "\n",
        "    num_pred = len(str(bot_responses[a]).split())  # the number of predicted tokens\n",
        "    num_gold = len(str(groundtruth[a]).split())  # the number of ground truth tokens\n",
        "\n",
        "    num_c.append(num_common)\n",
        "    num_c_sum = sum(num_c)\n",
        "\n",
        "    num_p.append(num_pred)\n",
        "    num_p_sum = sum(num_p)\n",
        "\n",
        "    num_g.append(num_gold)\n",
        "    num_g_sum = sum(num_g)\n",
        "\n",
        "# Calculate the precision, recall and f1-score\n",
        "precision = 1.0 * num_c_sum / (num_c_sum + (num_p_sum - num_c_sum))\n",
        "recall = 1.0 * num_c_sum / (num_c_sum + (num_g_sum - num_c_sum))\n",
        "f1_score = (2 * precision * recall) / (precision + recall)\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1_score}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tN1B0IoXhYjY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "# Create a DataFrame with the results\n",
        "results = pd.DataFrame({\n",
        "    \"Question\": test_questions,\n",
        "    \"Bot Response\": bot_responses,\n",
        "    \"Expected Response\": groundtruth,\n",
        "    \"Jaccard Similarity\": jaccard_similarities,\n",
        "    \"BLEU Score\": bleu_scores,\n",
        "    \"ROUGE-1\": rouge_1_scores,\n",
        "    \"ROUGE-2\": rouge_2_scores,\n",
        "    \"ROUGE-L\": rouge_l_scores,\n",
        "    \"Response Time\": response_times\n",
        "})\n",
        "\n",
        "# Save the results to a CSVfile\n",
        "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "results.to_csv(f'gpt2_results_{timestamp}.csv', index=False)\n",
        "print(\"CSV file 'gpt2_results.csv' has been created.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
